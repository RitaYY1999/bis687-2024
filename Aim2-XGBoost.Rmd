---
title: "Aim2-XGBoost"
output: html_document
date: "2024-04-05"
---

# Load the Dataset

```{r}
library(readxl) 
#setwd("/Users/gladyswang/Desktop/Spring_2024/DS_Capstone")
sickle_cell <- read.csv("/Users/yuchen/Desktop/687/cell/CSV/curesc_year3_v3.csv")
```

# Data Preprocessing
```{r}
# filter out variables with same value
print_columns_same_value <- function(df) {
  same_value_columns <- sapply(df, function(col) length(unique(col)) == 1)
  names(df)[same_value_columns]
}
variables_with_same_value <- print_columns_same_value(sickle_cell)
sickle_cell <- sickle_cell[, !names(sickle_cell) %in% variables_with_same_value]
```


```{r}
# extract pre-HCT variables
codebook <- read_excel("/Users/yuchen/Desktop/687/cell/CSV/Codebook 2021 Year 3.xlsx")
codebook_prehct <- codebook[codebook['HCT status'] == "pre-HCT",]
completeRows <- complete.cases(codebook_prehct["HCT status"])
codebook_prehct <- codebook_prehct[completeRows, ]
codebook_prehct[codebook_prehct$`Variable name` == "racegp", 'Variable name'] <- 'raceg'
names(sickle_cell) <- tolower(names(sickle_cell))
common_columns <- intersect(names(sickle_cell), codebook_prehct$`Variable name`)
sickle_cell_prehct <- sickle_cell[, common_columns]
names(sickle_cell_prehct) <- toupper(names(sickle_cell_prehct))
sickle_cell_prehct$ACSPSHI <- sickle_cell$acspshi
```

```{r}
# remove duplicate variables
duplicate_variables <- c("AGE", "YEARTX", "SCREUNIT", "SCRENUNT", "SALBUNIT", "SALBNUNT", "HB1UNPR")
sickle_cell_prehct <- sickle_cell_prehct[, !names(sickle_cell_prehct) %in% duplicate_variables]
```

```{r}
# remove variables with >= 60% missing values
missing_percentage <- function(df, missing_values = c(NA, 99, -9)) {
  total_values <- sapply(df, function(col) length(col))
  missing_count <- sapply(df, function(col) sum(col %in% missing_values))
  missing_percentage <- round((missing_count / total_values) * 100, 2)
  return(missing_percentage)
}

missing_percentages <- missing_percentage(sickle_cell_prehct)
variables_to_remove <- names(missing_percentages)[missing_percentages >= 60]
sickle_cell_cleaned <- sickle_cell_prehct[, !names(sickle_cell_prehct) %in% variables_to_remove]
```

```{r}
# remove samples with unrecorded ACSPSHI
sickle_cell_cleaned <- sickle_cell_cleaned[sickle_cell_cleaned$ACSPSHI != 99, ]
```

```{r}
library(missForest)

# Perform imputation (this might take some time for larger datasets)
col_names<-c("SCREATPR", "SCREAULN", "HB1PR", "INTSCREPR", "AGEGPFF")
sickle_cell_cleaned[setdiff(names(sickle_cell_cleaned), col_names)] <- lapply(sickle_cell_cleaned[setdiff(names(sickle_cell_cleaned), col_names)], factor)

imputed_data <- missForest(sickle_cell_cleaned)$ximp

sickle_cell_imputed <- imputed_data
```

### XGBoost

```{r}
# Get dataset
imputed_data <- imputed_data[, c("HCTCIGPF", "GVHD_FINAL", "CONDGRP_FINAL", "CONDGRPF", "SCATXRSN", 
                             "HB1PR", "SCREAULN", "SCREATPR", "YEARGPF", "DONORF", "ETHNICIT", 
                             "INTSCREPR", "HLA_FINAL", "VOC2YPR", "SNEPHRPR", "ACSPSHI")]
```

#### One-hot Encoding

```{r}
# Numeric Columns
col_names<-c("SCREATPR", "SCREAULN", "HB1PR", "INTSCREPR")
X <- imputed_data[,1:15]
columns_to_encode <- setdiff(names(X), col_names)
data_encoded <- dummyVars("~.", data = X[, columns_to_encode], fullRank = TRUE) %>%
  predict(newdata = X[, columns_to_encode])
# Combine encoded columns with the numeric columns
data_combined <- cbind(X[, col_names], data_encoded)
```

```{r}
set.seed(12315)
Y <- as.numeric(as.character(imputed_data$ACSPSHI))
X <- data_combined
#n <- nrow(imputed_data)
train_indices <- sample(1:nrow(imputed_data), 0.75 * nrow(imputed_data)) 
val_indices <- setdiff(1:nrow(imputed_data), train_indices) 
X_train <- X[train_indices, ]
Y_train <- Y[train_indices]
X_val <- X[val_indices, ]
Y_val <- Y[val_indices]

# Create xgb.DMatrix
dtrain <-xgb.DMatrix(as.matrix(X_train),label=Y_train)
dtest <- xgb.DMatrix(as.matrix(X_val),label=Y_val)
```

#### Tuning Parameter

```{r}
metric <- "AUC"
control <- trainControl(method = "cv", number = 5)
xgb_grid <- expand.grid(
  nrounds = c(100, 200, 300),  # Example values for number of boosting rounds
  max_depth = c(3, 6, 9),       # Example values for maximum tree depth
  eta = c(0.01, 0.1, 0.3),      # Example values for learning rate
  gamma = c(0, 0.1),       
  colsample_bytree = c(0.6, 0.8),  
  min_child_weight = c(1, 3),      
  subsample = c(0.6, 0.8)          
)
xgb_tune <- train(
  x = X_train,
  y = Y_train,
  method = "xgbTree",
  trControl = control,
  tuneGrid = xgb_grid,
  metric = metric,
  objective = "binary:logistic"
)
best_model <- xgb_tune$finalModel
print(best_model)
# Get best parameter: nrounds=100, max_depth=3,eta=0.1,gamma=0
```

#### Model Fitting

```{r}
set.seed(12315)
imbalance_ratio <- sum(Y_train == 0) / sum(Y_train == 1)

xgb_model <- xgboost(data = dtrain,
                     booster = "gbtree",
                     nrounds = 100, 
                     gamma = 0, 
                     objective = "binary:logistic",
                     eta = 0.1, 
                     max_depth = 3,
                     scale_pos_weight = imbalance_ratio)  
pred <- predict(xgb_model, as.matrix(X_val))
predicted_labels <- ifelse(pred > 0.5, 1, 0) 
```

#### Confusion matrix, sensitivity and specificity

```{r}
conf_matrix <- confusionMatrix(factor(predicted_labels), factor(Y_val))
print(conf_matrix)
```

#### ROC plot

```{r}
library(caret)
library(pROC)

roc <- roc(as.numeric(Y_val), as.numeric(pred))
auc <- auc(roc)
plot(roc, main = "ROC Curve", col = "blue", lwd = 2)
```
